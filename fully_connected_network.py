# -*- coding: utf-8 -*-
"""Fully Connected Network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1STMGA2Yw8ycI7usqGb55vzJB71xT-V8p

**FULLY CONNECTED NETWORK**

**Imports**
"""

# set working directory to google drive
from google.colab import drive
drive.mount('/content/drive')

from PIL import Image
import numpy as np
from matplotlib import pyplot as plt
import torch
import torchvision
import torch.nn as nn
from tqdm import tqdm 
from tqdm import trange 
from scipy.io import loadmat
from scipy import interpolate
import torch.nn.functional as F
from time import time 
from tqdm import tqdm
import copy

"""**Hyperparameters**"""

hidden_layers1=1000
hidden_layers2=100
hidden_layers3=10
batch_size=50
image_resolution=56
num_epochs=30

# if draw_errors=True, error elements are sampled from the difference distribution. Otherwise, no error elements are sampled and standard MAC operations are implemented. 
draw_errors= True  # False #

"""**Load Training and Testing Data**"""

trans=torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Resize([image_resolution,image_resolution])])

train_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('/files/', train=True, download=True,transform=trans),batch_size=batch_size, shuffle=True)

test_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('/files/', train=False, download=True,transform=trans),batch_size=batch_size, shuffle=False)

training_data=enumerate(train_loader)
testing_data=enumerate(test_loader)

"""**Determine the Number of Batches in the Testing Data**"""

counter=0
for test_img, test_gt in test_loader:
  counter+=1

num_batches_test_data=counter
print("there are"+" "+str(num_batches_test_data)+" "+"batches in the testing data")

train_counter=0
for train_img, train_gt in train_loader:
  train_counter +=1 

print("there are"+" "+str(train_counter)+" "+"batches in the training data")

"""**Show Samples from the Training and Testing Sets**"""

_,train_data=next(training_data)
###########################################
# one training batch
sample_train_img_batch=train_data[0]
sample_train_gt_batch=train_data[1]
print("training batch is of size"+" "+str(sample_train_img_batch.size()))
print("training batch gt is of size"+" "+str(sample_train_gt_batch.size()))

# one training sample
train_sample=sample_train_img_batch[0]
train_sample_gt=sample_train_gt_batch[0]
print("single training image is of size"+str(train_sample.size()))
print("corresponding gt is"+" "+str(train_sample_gt))
print()
print()
########################################
# one testing batch
_,test_data=next(testing_data)
sample_test_img_batch=test_data[0]
sample_test_gt_batch=test_data[1]
print("testing batch is of size"+" "+str(sample_test_img_batch.size()))
print("testing batch gt is of size"+" "+str(sample_test_gt_batch.size()))

# one testing sample
test_sample=sample_test_img_batch[0]
test_sample_gt=sample_test_gt_batch[0]
print("single test image is of size"+" "+str(test_sample.size()))
print("corresponding gt is"+" "+str(test_sample_gt))
print()
print()
print("some sample images are shown below")
plt.figure(figsize=(6,6))
plt.imshow(train_sample.squeeze())
plt.title("sample training image")

plt.figure(figsize=(6,6))
plt.imshow(test_sample.squeeze())
plt.title("sample test image")

"""**Find the Global *Minimum* from the test set (the set through which error will be propagated) in order to prove that the initial input to the network is in range(0,1)**"""

min_per_batch=[]
for input_test, input_target in test_loader:
  min_elem=(torch.min(input_test))
  min_per_batch.append(min_elem)

print("min input pixel value in the testing set is:"+" "+str(min(min_per_batch).item())+" "+"This is the input directly fed into the first connected layer")

"""**Construct Fully Connected Network** """

class MNIST_3(nn.Module):
  def __init__(self):
    super(MNIST_3, self).__init__()
    self.L1=torch.nn.Linear(in_features=image_resolution**2, out_features=hidden_layers1, bias=False)
    self.L2=torch.nn.Linear(in_features=hidden_layers1, out_features=hidden_layers2, bias=False)
    self.L3=torch.nn.Linear(in_features=hidden_layers2, out_features=hidden_layers3, bias=False)
  def forward(self, input):
    L1=self.L1(input)
    R1=nn.ReLU(inplace=True)(L1)
    L2=self.L2(R1)
    R2=nn.ReLU(inplace=True)(L2)
    L3=self.L3(R2)
    OUT=nn.Softmax()(L3)
    return OUT

"""**Run Sample Forward Pass Through Network** """

# instantiate network
FC3=MNIST_3()
# reshape input to be a vector
sample_nn_input=sample_train_img_batch.view(batch_size,image_resolution**2)
# forward pass
sample_nn_output=FC3(sample_nn_input)
sample_final_output=torch.argmax(sample_nn_output, dim=1)
print("sample output is"+" "+str(sample_final_output))

"""**define the number of testing batches to be used from the testing loader**"""

num_testing_batches=20

"""**Train Fully Connected Network and Save Trained Network** """

def train_FC3(model, optimizer, loss_func, scheduler):
  batch_loss=[]
  for epoch in trange(num_epochs):  # loop over the dataset multiple times
    model.train()
    running_loss = 0.0
    for input_batch,input_gt in train_loader: 
      with torch.set_grad_enabled(True):
        # vectorize image
        input_batch=input_batch.view(batch_size,image_resolution**2)
#        print("input batch"+" "+str(input_batch.size()))
#        input_gt=data_input[1]
#        print("gt size"+" "+str(input_gt.size()))
        # zero the parameter gradients
        optimizer.zero_grad()
        # forward pass
        output=model(input_batch)
#        print("output size"+" "+str(output.size()))
        # compute loss
        loss=loss_func(output,input_gt)
#        print(loss)
        # forward + backwprop + step optimizer
        loss.backward()
        optimizer.step()
        # print statistics
        running_loss += loss.item()
        batch_loss.append(loss)
#        if (j % 2) == 0:      # print every even batch
#          print("loss for epoch"+" "+str(epoch)+":"+" "+str(loss))
  print('Finished Training')
  return model, batch_loss

"""**Load and Test Trained Model**"""

#PATH='/content/drive/My Drive/FullyConnected_3LayerMNIST_batchsize100_resolution56_epochs30.pt'
PATH='/content/drive/My Drive/FullyConnected_3LayerMNIST_batchsize50_resolution56_epochs30.pt'
FC3_model=torch.load(PATH)
FC3_model.eval()

def test_FC_model(model):
  '''
  tests a model
  INPUT: a model of the form y=f(x) that maps an input to an output
  '''
  output_lst=[]
  gt_lst=[]
  test_counter=0
  per_batch_accuracy=[]
  for test_batch,test_gt in test_loader:
#    print("batch number"+" "+str(test_counter)+" "+"of"+" "+str(num_testing_batches))
    test_counter +=1
    if test_counter>num_testing_batches:
        break
    test_input=test_batch.view(batch_size,image_resolution**2)
    with torch.no_grad():
      test_out=model(test_input)
      final_out=torch.argmax(test_out, dim=1).numpy()
      output_lst.append(final_out)
      gt=test_gt.numpy()
      gt_lst.append(gt)

      # first, look at the accuracy per batch
      acc_per_batch_lst=[]
      for j in range(len(final_out)):
        if final_out[j]==gt[j]:
          acc_per_batch_lst.append(1)
      
      acc_per_batch=len(acc_per_batch_lst)/len(final_out)      
      per_batch_accuracy.append(acc_per_batch)

 # next, horizontally stack the outputs and ground truths into single arrays
  output_array=np.hstack(output_lst)
  gt_array=np.hstack(gt_lst)
  # compare the stacked output array with the stacked ground truth array
  correct_predictions=[]
  for i in range(len(output_array)):
    output_elem=output_array[i]
    gt_elem=gt_array[i]
    if output_elem==gt_elem:
      correct_predictions.append(1)
  
  final_accuracy=len(correct_predictions)/len(output_array)
  return final_accuracy, per_batch_accuracy



print()
print("accuracy of ground truth model is"+" "+str(test_FC_model(FC3_model)[0]))

"""**Print Model State Dictionary**"""

weight_list=[]
for param_tensor in FC3_model.state_dict():
  print(param_tensor, FC3_model.state_dict()[param_tensor].size())
  weight_list.append(FC3_model.state_dict()[param_tensor])

"""**Access Difference Distribution Data**"""

mat_path='/content/drive/My Drive/distribution_data.mat'
storage = loadmat(mat_path)
storage = storage['dd']
to_plot_storage = storage.flatten()
print(to_plot_storage.shape[0])

"""**Construct a Function that Randomly Draws from the Difference Distribution** """

def Inverse_CDF_to_sample(given_dist,length_CDF_data):
  '''
  NOTE: THIS FUNCTION IS DISTINCT FROM  randm_3Dmat, WHICH DRAWS AN DIFFERENCE DISTRIBUTION ELEMENTS TO BE ADDED
  TO THE HADAMARD PRODUCTS IN THE NETWORK. THE ELEMENTS SAMPLED FROM to_plot_storage USING THIS FUNCTION WILL FORM
  THE 3D ERROR VOLUME THAT randm_3Dmat WILL SAMPLE FROM.
  '''
  if draw_errors==True:
    sorted_data=np.sort(given_dist)
# map each sorted data point to a (uniform) index between 0 and 1
    uniform_indx=np.linspace(0,1,len(sorted_data))
    Inv_CDF=interpolate.interp1d(uniform_indx, sorted_data, kind='nearest')
  else:
    Inv_CDF= np.zeros(len(given_dist)*2000)
  return Inv_CDF


def randm_mat(CDF_Inverse,m,n):
  '''
  input...
  (m,n)=size of random error matrix to be drawn from the inverse cdf
  output...
  random error matrix of size [m,n]
  '''
  if draw_errors==True:
  #sorted_data=np.sort(given_dist)
    n_extracted=m*n
  #uniform_indx=np.linspace(0,1,len(sorted_data))
  #Inv_CDF=interpolate.interp1d(uniform_indx, sorted_data, kind='nearest')
    X_unif_to_sample=np.random.uniform(0,1,n_extracted)
    pdf_samples=CDF_Inverse(X_unif_to_sample)
    rand_mat=np.array(pdf_samples)
    rand_mat=np.reshape(rand_mat, (m,n))
  else:
    indices_to_sample=np.linspace(1,m*n,m*n).astype(int)
    indices_to_sample=np.reshape(indices_to_sample,(-1,1))
#    print(indices_to_sample.shape)
#    print(CDF_Inverse.shape)
    sampled_Inverse_CDF=CDF_Inverse[indices_to_sample]
    rand_mat=np.reshape(sampled_Inverse_CDF,(m,n))
  return rand_mat

inverse_cdf_to_sample=Inverse_CDF_to_sample(to_plot_storage,len(to_plot_storage))

sample=randm_mat(inverse_cdf_to_sample,100,100)

print("the max error element drawn is:"+" "+str(np.max(sample)))

def show_error_hist():
  '''
  This function plots the given error distribution from NETCAST:"to_plot_storage"
  ...along with the error distribution used for the propagation of error- to_plot"

  NOTE: only makes sense to run this function if perturbations are on, otherwise, it will return a histogram that looks like a spike at 0
  '''
  sampled_distribution=randm_mat(inverse_cdf_to_sample,1000,500).flatten()
  sampled_mean=np.mean(sampled_distribution)
  original_mean=np.mean(to_plot_storage)
  ######################################
  plt.figure(figsize=(5,5))
  plt.title("original error dist.")
  counts, bins,_=plt.hist(to_plot_storage, bins=int(len(to_plot_storage)/10), density=True)
  plt.ylim(0,200)
  #####################################
  plt.figure(figsize=(5,5))
  #points_to_analyze=randm_mat(to_plot,1,num_sampled_points_to_analyze).flatten() #np.random.choice(to_plot.flatten(),num_sampled_points_to_analyze)
  plt.title("sampled error dist.")
  _, _,_ =plt.hist(sampled_distribution, bins=int(len(sampled_distribution)/10), density=True)
  plt.ylim(0,200)
  #####################################
  print("error dist mean:"+" "+str(original_mean))
  print("mean of sampled error dist"+" "+str(sampled_mean))
  print("range of original error dist:"+str(np.min(to_plot_storage))+" "+str(np.max(to_plot_storage)))
  print("range of sampled error dist:"+str(np.min(sampled_distribution))+" "+str(np.max(sampled_distribution)))

show_error_hist()

"""**Construct Low Level Utility Functions**"""

def quick_split_W3D(matrix):
    '''
    This function splits a weight matrix, W, into two parts: W_+ and W_-, such that W=(W_+)+(W_-). This is done to mitigate
    error accumulation of the network's fully connected layer. Effectively, this offers an alternative to the previous method
    of matrix preprocessing, which accumulated excess error by performing element-wise addition in the weight matrix by its 
    max value to get it in range [0,a]

    INPUT: matrix, a tensor from the state dictionary of the trained model
    '''
    # form the negative of the input matrix
    negative_matrix=-1*matrix
    # use RELU to find the positive matrix W_+
    W_pos=nn.ReLU(inplace=True)(torch.tensor(matrix)).numpy()
    # now, use RELU of the **negative matrix** to find the negative factored matrix W_-
    W_neg=nn.ReLU(inplace=True)(torch.tensor(negative_matrix)).numpy()
    return torch.tensor(W_pos), torch.tensor(W_neg)


def Matrix_Multiply_with_diff_dist(inverse_of_cdf,X,W):
  '''
  performs a simulated matrix product between two 2d tensors
  X: 2d input tensor of size [Batch_number, N]
  W: 2d weight tensor of size [M,N]
  **NOTE**: THIS ASSUMES THAT X AND W ARE BOTH PREPROCESSED TO BE IN THE RANGE [0,1]
  '''
  #X=X.numpy()
  #W=W.numpy()

  X_lst=[]
# loop through rows of the input to form the 3D input tensor
  for row_indx in range(X.shape[0]):
    row_to_copy=X[row_indx]
    matrix_from_row=np.tile(row_to_copy,(W.shape[0],1))
    X_lst.append(matrix_from_row)

  X_vstacked=np.vstack(X_lst)

  stacked_weight_matrices=[]
  for matrix_indx in range(X.shape[0]):
    weight_matrix_to_stack=W
    stacked_weight_matrices.append(weight_matrix_to_stack)
 
  W_vstacked=np.vstack(stacked_weight_matrices)
  # FORM THE HADAMARD PRODUCT 
  Hadamard_prod=W_vstacked*X_vstacked

  if np.min(Hadamard_prod)<0 or np.max(Hadamard_prod)>1:
    print("ERROR: Hadamard Product is not in rang e[0,1]")
  # ADD MATRIX OF ERRORS TO HADAMARD PRODUCT

  #print(np.max(randm_mat(inverse_of_cdf, Hadamard_prod.shape[0], Hadamard_prod.shape[1])))

  Hadamard_prod_with_errors=Hadamard_prod + randm_mat(inverse_of_cdf, Hadamard_prod.shape[0], Hadamard_prod.shape[1])
  # SUM HADAMARD PRODUCT TO FORM MATRIX PRODUCT
  summed_Hadamard_prod=np.sum(Hadamard_prod_with_errors, axis=1)
  matrix_prod=np.reshape(summed_Hadamard_prod,(X.shape[0],W.shape[0]))
  return torch.tensor(matrix_prod)

"""**Begin Error Propagation**
=====================================

**Define Error Propagation Function**
"""

def Propagate_Error_FC3(input_image_batch):
  '''
  simulates FC3 with drawn difference distribution errors.
  INPUT: a batch of image inputs
  '''
  for k in trange(3):
    W=weight_list[k]
  # (I) linear part of FC layer
    if k==0:
      X=input_image_batch
  # (I.1) preprocess the input and weight matrices by dividing each by their respective maximum values
    n_x=torch.max(X)
    X_pre=X/n_x
    c= torch.min(W)
    d= torch.max(W)
    n_w=max(torch.abs(c),torch.abs(d))
    W_pre=W/n_w
  # (I.2) split W_pre into two matrices both in the range [0,1] as follows: W_pre --> W_+, W_- 
  # where we will factor as follows: W_pre=W_plus-W_minus
    W_plus, W_minus=quick_split_W3D(W_pre)
  # (I.3) check that X_pre, W_plus, and W_minus are all in the range [0,1]
    if torch.min(X_pre)<0 or torch.max(X_pre)>1:
      raise Exception("ERROR: X_pre is not in [0,1]")
    if torch.min(W_plus)<0 or torch.max(W_plus)>1:
      raise Exception("ERROR: W_plus is not in [0,1]")
    if  torch.min(W_minus)<0 or torch.max(W_minus)>1:
      raise Exception("ERROR: W_minus is not in [0,1]")
  # (I.4) form the matrix product with difference distribution errors
  # (I.4.a) X_pre @ W_plus with added difference distribution elements
    X_pre_W_plus = Matrix_Multiply_with_diff_dist(inverse_cdf_to_sample,X_pre.detach().numpy(),W_plus.detach().numpy())
  # (I.4.b) X_pre @ W_minus with added difference distribution elements
    X_pre_W_minus = Matrix_Multiply_with_diff_dist(inverse_cdf_to_sample,X_pre.detach().numpy(),W_minus.detach().numpy())
  # (I.4.c) subtract the products and post-process by multiplying by the normalization factors
    X_linear_product=n_x*n_w*(X_pre_W_plus-X_pre_W_minus )

  # (II) non-linear activation function
    if k==2:
      activation_fn=nn.Softmax()
    else:
      activation_fn=nn.ReLU(inplace=True)

    X_final_product=activation_fn(X_linear_product)

  # (III) feed output into next layer of the network
    X = X_final_product
  return X_final_product

"""**Propagate Error Through FC3**"""

test=test_FC_model(model=Propagate_Error_FC3)
acc=test[0]
per_batch_acc_list=test[1]

"""**Calculate Histogram of Perturbed Accuracies**"""

acc_array= per_batch_acc_list  #[0.99, 0.98, 0.99, 0.99, 0.99, 0.99, 0.98, 0.99, 0.99, 0.98, 0.99, 0.99, 0.98, 0.99, 0.99, 0.99, 0.98, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.98, 0.99, 0.99, 0.99, 0.99, 0.98, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.98, 0.98, 0.98, 0.98, 0.99, 0.99, 0.99, 0.98, 0.98, 0.99, 0.99, 0.99, 0.98, 0.99, 0.98, 0.99, 0.98, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.98, 0.99, 0.98, 0.99, 0.99, 0.99, 0.99, 0.98, 0.99, 0.99, 0.99, 0.99, 0.98, 0.98, 0.98, 0.99, 0.99, 0.98, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99]

histogram=plt.hist(acc_array, bins=int(4), density=False)
plt.title("Histogram of NETCAST accuracy over several iterations")
plt.show()

print()
print("average perturbed accuracy:"+" "+str(acc))
print(np.average(acc_array))
# -*- coding: utf-8 -*-
"""ONN_Simulation_For_Scene_Recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sOL-O9XG-RgTiFwUHPqMqj5a5Ad50JNb

**Set CWD to Google Drive**
"""

from google.colab import drive
drive.mount('/content/drive')

from PIL import Image, ImageEnhance
import numpy as np
from matplotlib import pyplot as plt
import torch
#print(torch.__version__)
import torchvision
#print(torchvision.__version__)
import torch.nn as nn
from tqdm import tqdm 
from tqdm import trange 
from scipy.io import loadmat
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from matplotlib import cm
from numpy import asarray
import random
from time import time
from scipy import interpolate
from torchvision.datasets import MNIST
from torchvision.datasets.mnist import read_image_file,read_label_file
from torchvision.datasets.utils import extract_archive
from typing import Optional,Callable
import os
import h5py
from torchvision.transforms import Resize, Compose, ToTensor, Normalize
from scipy import stats
from scipy.signal import butter, lfilter, freqz
from scipy import ndimage

"""**Define Global Variables** """

train_batchsize=32
val_batchsize=32
test_batchsize=32

resolution= 168   # ***ASSUMES A SQUARE IMAGE

draw_errors= True #False

def LoadImage(file_dir):
  file_name_list = sorted(os.listdir(file_dir))
  image_list = []
  for file_name in file_name_list:
    file_path = os.path.join(file_dir, file_name)
    file_image = Image.open(file_path)
    image_list.append(file_image)
  return image_list, file_name_list

  
class ImageDataset():
  def __init__(self,  input_dir1, trans):
    self.input_images,_ = LoadImage(input_dir1)
    self.transform = trans
  def __len__(self):
    return len(self.input_images)
  def __getitem__(self, idx):
    image = np.array((self.input_images[idx]))
    if len(list(image.shape))<3:
      image=np.reshape(image,(image.shape[0],image.shape[1],1))
    shape_list=[image.shape[0],image.shape[1],image.shape[2]]
 #   if image.shape[2] !=3: 
 #     print("error in processed image")
 #     print("erroroneous image size is...")
 #     print(image.shape)

    if self.transform:
      image=transforms.ToTensor()(image.astype('uint8'))
      if image.size()[0]!=3:
        print(image.size()[0])
  #    image=Image.fromarray(image,mode='L')
      image = self.transform(image)  
    return image
      

class PairedDataset(Dataset):
    def __init__(self, images, labels):
      self.images=images
      self.labels=labels
    def __len__(self):
      return len(self.images)
    def __getitem__(self, indx):
      image= self.images[indx]
      label=self.labels[indx]   
      return image, label


trans= transforms.Resize([resolution,resolution])

"""**Define Low level functions**"""

def Inverse_CDF_to_sample(given_dist,length_CDF_data):
  '''
  NOTE: THIS FUNCTION IS DISTINCT FROM  randm_3Dmat, WHICH DRAWS AN DIFFERENCE DISTRIBUTION ELEMENTS TO BE ADDED
  TO THE HADAMARD PRODUCTS IN THE NETWORK. THE ELEMENTS SAMPLED FROM to_plot_storage USING THIS FUNCTION WILL FORM
  THE 3D ERROR VOLUME THAT randm_3Dmat WILL SAMPLE FROM.
  '''
  if draw_errors==True:
    sorted_data=np.sort(given_dist)
# map each sorted data point to a (uniform) index between 0 and 1
    uniform_indx=np.linspace(0,1,len(sorted_data))
    Inv_CDF=interpolate.interp1d(uniform_indx, sorted_data, kind='nearest')
  else:
    Inv_CDF=given_dist
  return Inv_CDF


def draw_3D_matrix_from_3D_diffdist(Diff_volume,k,m,n):
  start_indx0=np.random.randint(0,Diff_volume.shape[0]-k+1)
  start_indx1=np.random.randint(0,Diff_volume.shape[1]-m+1)
  start_indx2=np.random.randint(0,Diff_volume.shape[2]-n+1)
  return np.array(Diff_volume[start_indx0:start_indx0+k,start_indx1:start_indx1+m, start_indx2:start_indx2+n])

def draw_2D_Matrix_from_3D_diffdist(Diff_vol,m,n):
  '''
  diff volume here is 2d
  '''
  #channel_index=np.random.randint(0,Diff_vol.shape[2]+1)
  idx_0=np.random.randint(0,Diff_vol.shape[0]-m+1)
  idx_1=np.random.randint(0,Diff_vol.shape[1]-n+1)
  return np.array(Diff_vol[idx_0:idx_0+m, idx_1:idx_1+n])


def randm_3Dmat(Inverse_CDF,k,m,n):
  '''
  input...
  (k, m,n)=size of random error 3D matrix to be drawn from the given distribution
  output...
  random 3D error matrix of size [k,m,n]
  '''
  if draw_errors==True:
    num_elements_to_extract=k*m*n
    X_unif_to_sample=np.random.uniform(0,1,num_elements_to_extract)
    pdf_samples=Inverse_CDF(X_unif_to_sample)
    Three_D_mat=np.reshape(pdf_samples,(k,m,n))
  else:
    indices_to_sample=np.linspace(1,k*m*n,k*m*n).astype(int)
    indices_to_sample=np.reshape(indices_to_sample,(-1,1))
    sampled_Inverse_CDF=Inverse_CDF[indices_to_sample]
    Three_D_mat=np.reshape(sampled_Inverse_CDF,(k,m,n))
  return Three_D_mat


def quick_split_W3D(matrix):
    '''
    This function splits a weight matrix, W, into two parts: W_+ and W_-, such that W=(W_+)+(W_-). This is done to mitigate
    error accumulation of the network's fully connected layer. Effectively, this offers an alternative to the previous method
    of matrix preprocessing, which accumulated excess error by performing element-wise addition in the weight matrix by its 
    max value to get it in range [0,a]

    INPUT: matrix, a tensor from the state dictionary of the trained model
    '''
    # form the negative of the input matrix
    negative_matrix=-1*matrix
    # use RELU to find the positive matrix W_+
    W_pos=nn.ReLU(inplace=True)(torch.tensor(matrix)).numpy()
    # now, use RELU of the **negative matrix** to find the negative factored matrix W_-
    W_neg=nn.ReLU(inplace=True)(torch.tensor(negative_matrix)).numpy()
    return W_pos, W_neg

def Three_D_Conv_withdiff_Layer2(cdf_inverse,inp,weight,s):
  '''
  NOTE: THIS FUNCTION PERFORMS THE SAME FUNCTIONALITY AS A Conv_2d layer IN PYTORCH BUT INCORPORATES ERROR ELEMENTS FROM THE DIFFERENCE 
  DISTRIBUTION

  The difference distribution elements are drawn from the inverse cdf

  input: 3D padded input array of the form [C,H,W] (padding will be applied prior to Conv_4D)
  weight: 3D weight kernel 

  output: 2d array corresponding to the convolution between the 3D input and the 3D weight kernel
  '''
  Two_d_lst=[]
  i_range=(inp.shape[1]-weight.shape[1])/s +1
  j_range=(inp.shape[2]-weight.shape[2])/s +1
  if 100 * (i_range - int(i_range))!= 0.0:
    print("ERROR")
  if 100 * (j_range - int(j_range))!= 0.0:
    print("ERROR")

  for i in range(int(i_range)):
      for j in range(int(j_range)):
          X_ijk=inp[:,i*s:i*s+weight.shape[1],j*s:j*s+weight.shape[2]]
          # preprocess input element (X_ijk)
          N_X=np.max(X_ijk)
          if N_X==0:
            N_X=1
          X_ijk_pre=X_ijk/N_X
          # preprocess weight element (weight)
          c=np.min(weight)
          d=np.max(weight)
          N_W=max(np.abs(c),np.abs(d))
          if N_W==0:
            N_W=1
          weight_pre=weight/N_W
          # split the weight into the K_+ and K_- components
          K_plus, K_minus = quick_split_W3D(weight_pre)
          # form the Hadamard products
          prod_plus=X_ijk_pre*K_plus
          prod_minus=X_ijk_pre*K_minus
          # check to ensure that the Hadamard partial products and pre-processed input is in the range [0,1]
  #        if np.min(prod_plus)<0 or np.max(prod_plus)>1:
  #          raise Exception("ERROR: partial product X(ijk)*K_+ not in range [0,1]")
  #        if np.min(prod_minus)<0 or np.max(prod_minus)>1:
  #          raise Exception("ERROR: partial product X(ijk)*K_- not in range [0,1]")
          # *****ADD PERTURBATION (matrix) to the element-to-element products
          prod_plus_with_diff = prod_plus + randm_3Dmat(cdf_inverse,prod_plus.shape[0],prod_plus.shape[1],prod_plus.shape[2])
          prod_minus_with_diff = prod_minus + randm_3Dmat(cdf_inverse,prod_minus.shape[0],prod_minus.shape[1],prod_minus.shape[2])
          # post-process by subtracting: X(ijk)K_+ - X(ijk)K_- and multiplying by the normalization factors
          prod_C_post_processed= N_X*N_W*(prod_plus_with_diff-prod_minus_with_diff)
          prod_C2=np.sum(prod_C_post_processed)       
          Two_d_lst.append(prod_C2)
  Two_d_lst=np.array(Two_d_lst)
  Two_d_lst=np.reshape(Two_d_lst,(int((inp.shape[1]-weight.shape[1])/s +1),int((inp.shape[2]-weight.shape[2])/s +1)))
  return Two_d_lst

def Three_D_Conv_with_volume_error(error_volume,inp,weight,s):
  '''
  NOTE: THIS FUNCTION PERFORMS THE SAME FUNCTIONALITY AS A Conv_2d layer IN PYTORCH BUT INCORPORATES ERROR ELEMENTS FROM THE DIFFERENCE 
  DISTRIBUTION

  The difference distribution elements are drawn from the inverse cdf

  input: 3D padded input array of the form [C,H,W] (padding will be applied prior to Conv_4D)
  weight: 3D weight kernel 

  output: 2d array corresponding to the convolution between the 3D input and the 3D weight kernel
  '''
  Two_d_lst=[]
  i_range=(inp.shape[1]-weight.shape[1])/s +1
  j_range=(inp.shape[2]-weight.shape[2])/s +1
  if 100 * (i_range - int(i_range))!= 0.0:
    print("ERROR")
  if 100 * (j_range - int(j_range))!= 0.0:
    print("ERROR")
  for i in range(int(i_range)):
      for j in range(int(j_range)):
          X_ijk=inp[:,i*s:i*s+weight.shape[1],j*s:j*s+weight.shape[2]]
          # preprocess input element (X_ijk)
          N_X=np.max(X_ijk)
          if N_X==0:
            N_X=1
          X_ijk_pre=X_ijk/N_X
          # preprocess weight element (weight)
          c=np.min(weight)
          d=np.max(weight)
          N_W=max(np.abs(c),np.abs(d))
          if N_W==0:
            N_W=1
          weight_pre=weight/N_W
          # split the weight into the K_+ and K_- components
          K_plus, K_minus = quick_split_W3D(weight_pre)
          # form the Hadamard products
          prod_plus=X_ijk_pre*K_plus
          prod_minus=X_ijk_pre*K_minus
          # check to ensure that the Hadamard partial products and pre-processed input is in the range [0,1]
          if np.min(prod_plus)<0 or np.max(prod_plus)>1:
            raise Exception("ERROR: partial product X(ijk)*K_+ not in range [0,1]")
          if np.min(prod_minus)<0 or np.max(prod_minus)>1:
            raise Exception("ERROR: partial product X(ijk)*K_- not in range [0,1]")
          # *****ADD PERTURBATION (matrix) to the element-to-element products
          prod_plus_with_diff = prod_plus 
          prod_minus_with_diff = prod_minus 
          # post-process by subtracting: X(ijk)K_+ - X(ijk)K_- and multiplying by the normalization factors
          prod_C_post_processed= N_X*N_W*(prod_plus_with_diff-prod_minus_with_diff)
          prod_C2=np.sum(prod_C_post_processed)       
          Two_d_lst.append(prod_C2)
  Two_d_lst=np.array(Two_d_lst)
  Two_d_lst=np.reshape(Two_d_lst,(int((inp.shape[1]-weight.shape[1])/s +1),int((inp.shape[2]-weight.shape[2])/s +1)))
  return Two_d_lst


def Conv_4D_With_Processing_Layer2(cdf_inverse,x,w,s):
  '''
  This function uses Three_D_Conv and preprocess_mat to compute a single convolution between a 4D input NUMPY ARRAY
  and a 4D weight NUMPY ARRAY. It returns the 4D convolved NUMPY ARRAY.
  INPUTS...
  x: 4D input (numpy array) **PADDED**!
  w: 4D weight (numpy array)
  s: stride
  '''
  Four_d_stack_lst=[]
  for img_indx in trange(x.shape[0]):
    image=x[img_indx]  # 3D IMAGE 
    Three_d_stack_lst=[]
    for kernel_indx in range(w.shape[0]):
      kernel=w[kernel_indx]
      shape0=int((image.shape[1]-kernel.shape[1])/s +1)
      shape1=int((image.shape[2]-kernel.shape[2])/s +1)
      kernel=w[kernel_indx] # 3D KERNEL
      Conv3D=Three_D_Conv_withdiff_Layer2(cdf_inverse,image,kernel,s)
      Three_d_stack_lst.append(Conv3D)
    Three_d_stack=np.reshape(Three_d_stack_lst,(w.shape[0],shape0,shape1))
    Four_d_stack_lst.append(Three_d_stack)
  Post_Processed_Res=np.reshape(Four_d_stack_lst,(x.shape[0],w.shape[0],shape0,shape1)) 
  return Post_Processed_Res


def Conv4d_with_3d_diff_dist(error_volume,x,w,s):
  Four_d_stack_lst=[]
  for img_indx in trange(x.shape[0]):
    image=x[img_indx]  # 3D IMAGE 
    Three_d_stack_lst=[]
    for kernel_indx in range(w.shape[0]):
      kernel=w[kernel_indx]
      shape0=int((image.shape[1]-kernel.shape[1])/s +1)
      shape1=int((image.shape[2]-kernel.shape[2])/s +1)
      kernel=w[kernel_indx] # 3D KERNEL
      Conv3D=Three_D_Conv_with_volume_error(error_volume,image,kernel,s)  
      Three_d_stack_lst.append(Conv3D)
    Three_d_stack=np.reshape(Three_d_stack_lst,(w.shape[0],shape0,shape1))
    Four_d_stack_lst.append(Three_d_stack)
  Post_Processed_Res=np.reshape(Four_d_stack_lst,(x.shape[0],w.shape[0],shape0,shape1)) 
  return Post_Processed_Res



def split_W2D(matrix):
    '''
    This function splits a weight matrix, W, into two parts: W_+ and W_-, such that W=(W_+)+(W_-). This is done to mitigate
    error accumulation of the network's fully connected layer. Effectively, this offers an alternative to the previous method
    of matrix preprocessing, which accumulated excess error by performing element-wise addition in the weight matrix by its 
    max value to get it in range [0,a]
    '''
    W_pos_lst=[]
    W_neg_lst=[]
    for i in range(matrix.shape[0]):
      for j in range(matrix.shape[1]):
        if matrix[i,j]>=0:
          W_pos_lst.append(matrix[i,j])
          W_neg_lst.append(0)           
        elif matrix[i,j]<0:
          W_neg_lst.append(np.abs(matrix[i,j]))
          W_pos_lst.append(0)
    W_pos=np.array(W_pos_lst)
    W_neg=np.array(W_neg_lst) 
    W_pos=np.reshape(W_pos,(matrix.shape[0],matrix.shape[1]))
    W_neg=np.reshape(W_neg,(matrix.shape[0],matrix.shape[1]))
    return torch.tensor(W_pos), torch.tensor(W_neg)

def randm_mat(CDF_Inverse,m,n):
  '''
  input...
  (m,n)=size of random error matrix to be drawn from the difference distribution, to_plot
  output...
  random error matrix of size [m,n]
  '''
  if draw_errors==True:
  #sorted_data=np.sort(given_dist)
    n_extracted=m*n
  #uniform_indx=np.linspace(0,1,len(sorted_data))
  #Inv_CDF=interpolate.interp1d(uniform_indx, sorted_data, kind='nearest')
    X_unif_to_sample=np.random.uniform(0,1,n_extracted)
    pdf_samples=CDF_Inverse(X_unif_to_sample)
    rand_mat=np.array(pdf_samples)
    rand_mat=np.reshape(rand_mat, (m,n))
  else:
    indices_to_sample=np.linspace(1,m*n,m*n).astype(int)
    indices_to_sample=np.reshape(indices_to_sample,(-1,1))
    sampled_Inverse_CDF=CDF_Inverse[indices_to_sample]
    rand_mat=np.reshape(sampled_Inverse_CDF,(m,n))
  return rand_mat


def Matrix_Multiply_with_diff_dist(inverse_of_cdf,X,W):
  '''
  performs a simulated matrix product between two 2d tensors
  X: 2d input tensor of size [Batch_number, N]
  W: 2d weight tensor of size [M,N]
  **NOTE**: THIS ASSUMES THAT X AND W ARE BOTH PREPROCESSED TO BE IN THE RANGE [0,1]
  '''
  #X=X.numpy()
  #W=W.numpy()

  X_lst=[]
# loop through rows of the input to form the 3D input tensor
  for row_indx in range(X.shape[0]):
    row_to_copy=X[row_indx]
    matrix_from_row=np.tile(row_to_copy,(W.shape[0],1))
    X_lst.append(matrix_from_row)

  X_vstacked=np.vstack(X_lst)

  stacked_weight_matrices=[]
  for matrix_indx in range(X.shape[0]):
    weight_matrix_to_stack=W
    stacked_weight_matrices.append(weight_matrix_to_stack)
 
  W_vstacked=np.vstack(stacked_weight_matrices)
  # FORM THE HADAMARD PRODUCT 
  Hadamard_prod=W_vstacked*X_vstacked
  # ADD MATRIX OF ERRORS TO HADAMARD PRODUCT

  print(np.max(randm_mat(inverse_of_cdf, Hadamard_prod.shape[0], Hadamard_prod.shape[1])))

  Hadamard_prod_with_errors=Hadamard_prod + randm_mat(inverse_of_cdf, Hadamard_prod.shape[0], Hadamard_prod.shape[1])
  # SUM HADAMARD PRODUCT TO FORM MATRIX PRODUCT
  summed_Hadamard_prod=np.sum(Hadamard_prod_with_errors, axis=1)
  matrix_prod=np.reshape(summed_Hadamard_prod,(X.shape[0],W.shape[0]))
  return torch.tensor(matrix_prod)


def Matmul_with_2d_diffdist_elements(error_surface,X,W):
  '''
  performs a simulated matrix product between two 2d tensors
  X: 2d input tensor of size [Batch_number, N]
  W: 2d weight tensor of size [M,N]
  **NOTE**: THIS ASSUMES THAT X AND W ARE BOTH PREPROCESSED TO BE IN THE RANGE [0,1]
  '''
  #X=X.numpy()
  #W=W.numpy()

  X_lst=[]
# loop through rows of the input to form the 3D input tensor
  for row_indx in range(X.shape[0]):
    row_to_copy=X[row_indx]
    matrix_from_row=np.tile(row_to_copy,(W.shape[0],1))
    X_lst.append(matrix_from_row)

  X_vstacked=np.vstack(X_lst)

  stacked_weight_matrices=[]
  for matrix_indx in range(X.shape[0]):
    weight_matrix_to_stack=W
    stacked_weight_matrices.append(weight_matrix_to_stack)
 
  W_vstacked=np.vstack(stacked_weight_matrices)
  # FORM THE HADAMARD PRODUCT 
  Hadamard_prod=W_vstacked*X_vstacked
  if np.min(Hadamard_prod)<0 or np.max(Hadamard_prod)>1:
    print([np.min(Hadamard_prod),np.max(Hadamard_prod)>1])
    print("ERROR: Hadamard product in fully connected layer not in [0,1]")
  # ADD MATRIX OF ERRORS TO HADAMARD PRODUCT

  Hadamard_prod_with_errors=Hadamard_prod + draw_2D_Matrix_from_3D_diffdist(error_surface, Hadamard_prod.shape[0], Hadamard_prod.shape[1]) 
  # SUM HADAMARD PRODUCT TO FORM MATRIX PRODUCT
  summed_Hadamard_prod=np.sum(Hadamard_prod_with_errors, axis=1)
  matrix_prod=np.reshape(summed_Hadamard_prod,(X.shape[0],W.shape[0]))
  return torch.tensor(matrix_prod)

"""**Define Fast Convolution Function** """

def ADV_Conv_4D_With_Processing(cdf_inverse_obj,x,w,s,h_padding,w_padding,d_h,d_w):
  '''
  *****NOTE****: TO DRAW FROM A DISTRIBUTION OF ZEROS, SET cdf_inverse=CONV_Diff_Vol, and use 'draw_3D_matrix_from_3D_diffdist' instead of 'randm_3Dmat'

  This function uses Three_D_Conv and preprocess_mat to compute a single convolution between a 4D input NUMPY ARRAY
  and a 4D weight NUMPY ARRAY. It returns the 4D convolved NUMPY ARRAY.
  INPUTS...
  x: 4D input (TENSOR) 
  w: 4D weight (TENSOR)
  s: stride
  '''
  ##8 CONVERT FROM TENSOR TO NUMPY...
  x = x.detach().numpy()
  w = w.detach().numpy() 
  ################################### 

  #*******************form the dilated weight kernel*****************************#
  dilated_kernel_lst=[]
  for kernel_indx in range(w.shape[0]):
      kernel=w[kernel_indx]
      weight_depth=kernel.shape[0]
      init_kernel_h = kernel.shape[1]
      init_kernel_w = kernel.shape[2]
      # form the dimension of the dilated kernel
      dil_kernel_h = init_kernel_h + (init_kernel_h-1)*(d_h-1)
      dil_kernel_w = init_kernel_w + (init_kernel_w-1)*(d_w-1)
      # form a blank 3D dilated kernel to be filled in at the appropriate indices
      dilated_kernel = np.zeros((weight_depth, dil_kernel_h, dil_kernel_w)) # **to be filled in...
      # generate index map from the initial (non-dilated) kernel to the final (dilated) kernel...
      h_range=list(range(0,init_kernel_h))
      w_range=list(range(0,init_kernel_w))
      init_indx_pairs=np.array(np.meshgrid(h_range,w_range)).T.reshape(-1, 2) 
      for (a,b) in init_indx_pairs:
        a_dil=a*d_h
        b_dil=b*d_w
        dilated_kernel[:,a_dil,b_dil]=kernel[:,a,b]
      # over-write the kernel in all following lines of code to be the dilated kernel
      #kernel=dilated_kernel
      dilated_kernel_lst.append(dilated_kernel)
  Four_d_dilated_kernel=np.stack(dilated_kernel_lst,axis=0)
  ################################################################################
  # set the weight kernel, w, to equal the dilated kernel for all future computation
  w=Four_d_dilated_kernel
  Four_d_stack_lst=[]
  # loop through each 3D image  
  stacked_4d_image_lst=[]
  num_times_to_copy_x=w.shape[0]
  num_time_to_copy_w=x.shape[0]
  total_num_stacked_inputs=num_times_to_copy_x*num_time_to_copy_w
  for img_indx in range(x.shape[0]):
    image=x[img_indx]  # 3D IMAGE 
    # pad 3D image
    image=nn.ConstantPad2d((w_padding, w_padding, h_padding, h_padding),0)(torch.tensor(image)).numpy()
    # copy/stack (in the height dimension)
    stacked_image=np.tile(image,(num_times_to_copy_x,1))
    stacked_4d_image_lst.append(stacked_image)
    #
  stacked_4d_image=np.hstack(stacked_4d_image_lst)
  # copy/stack filter with the appropriate paddings inserted 
  Top_padding=image.shape[1]-w.shape[2]
  padding_function=nn.ConstantPad2d((0,0,Top_padding,0), 0)
  padd_function_bottom=nn.ConstantPad2d((0,0,0,Top_padding), 0) 
  stacked_weight_lst=[]
  nth_stacked_weight_lst=[]
  #############################################################################################################
  list_of_indices_to_del=[]
  first_list_of_indices_to_delete=[]
  first_index_to_delete=int(w.shape[2])
  first_list_of_indices_to_delete.append(first_index_to_delete)
  for c in range(Top_padding-1):
    first_index_to_delete+=1
    first_list_of_indices_to_delete.append(int(first_index_to_delete))
  list_of_indices_to_del.append(first_list_of_indices_to_delete)
  for d in range(total_num_stacked_inputs-2):
  # if we have 'n' stacked inputs, then we have 'n-1' zero fillers in between, and we already have the index list for one of them, so we repeat the below proceedure n-1-1=n-2 times
    lst_to_append=list(np.array(first_list_of_indices_to_delete)+w.shape[2]+Top_padding)
    list_of_indices_to_del.append(lst_to_append)
    first_list_of_indices_to_delete=lst_to_append
  list_of_indices_to_del=list(np.array(list_of_indices_to_del).flatten())
  ################################################################################################################
  for k in range(w.shape[0]):
    if k==0:
      stacked_weight_lst.append(w[0]) 
      nth_stacked_weight_lst.append(w[0])
    else:
      padded_weight=padding_function(torch.tensor(w[k])).numpy()
      stacked_weight_lst.append(padded_weight)
      nth_stacked_weight_lst.append(padded_weight)
      # no bottom padding
      nth_padded=padding_function(torch.tensor(w[k])).numpy()
      nth_stacked_weight_lst.append(nth_padded)
  stacked_4d_filter=np.hstack(stacked_weight_lst)
  stacked_4d_filter=padd_function_bottom(torch.tensor( stacked_4d_filter)).numpy()
  nth_padded_bank=np.hstack(nth_stacked_weight_lst)
  tiled_filter_bank=np.tile(stacked_4d_filter,(num_time_to_copy_w,1))
  num_padded_rows=Top_padding
  k_0=tiled_filter_bank.shape[1]-1
  indices_to_del=[k_0]
  for a in range(num_padded_rows-1):
    indices_to_del.append(k_0-1)
    k_0=k_0-1  
  # delete bottom block of zeros in place
  tiled_filter_bank=np.delete(tiled_filter_bank,indices_to_del,1)
###########################################################################
#** form the 3d bernouli mask in the same shape as the tiled filter bank
  bernouli_ones=torch.ones([tiled_filter_bank.shape[0],tiled_filter_bank.shape[1],tiled_filter_bank.shape[2]]).numpy()
# for each index to delete, set that sub-block equalt to zeros
  for index_to_zero in list_of_indices_to_del:
    bernouli_ones[:,index_to_zero]=0
###########################################################################
# the two 3D tensors to be convolved are: stacked_4d_image and tiled_filter_bank
  i_range=(stacked_4d_image.shape[1]-tiled_filter_bank.shape[1])/s +1
  j_range=(stacked_4d_image.shape[2]- tiled_filter_bank.shape[2])/s +1
  lateral_size = tiled_filter_bank.shape[2] 
  stacked_conv_block_lst=[]

  for i in trange(int(i_range)):
      for j in range(int(j_range)):
        # perform the lateral convolutions...
        x_ijk_lateral_conv=stacked_4d_image[:,i*s:i*s+tiled_filter_bank.shape[1],j*s:j*s+tiled_filter_bank.shape[2]]  

        ############################################################################################################################################## 
        # pre-process x_ijk by dividing by it by its maximum value
        n_x = np.max(x_ijk_lateral_conv)  
        if n_x==0:
          n_x=1    
        x_ijk_pre=x_ijk_lateral_conv/n_x
        # pre-process the filter stack 
        c=np.min(tiled_filter_bank) 
        d=np.max(tiled_filter_bank) 
        n_w=max(np.abs(c),np.abs(d))
        if n_w==0:
          n_w=1
        filter_pre=tiled_filter_bank/n_w
        stacked_filter_plus, stacked_filter_minus = quick_split_W3D(filter_pre)
   #     tiled_filter_bank_pre, a, n_w=matrix_preprocess(tiled_filter_bank)
        # perform the + and -Hadamard product
        Hadamard_prod_plus= x_ijk_pre*stacked_filter_plus  #x_ijk_lateral_conv*tiled_filter_bank
        Hadamard_prod_minus= x_ijk_pre*stacked_filter_minus

  #      print(Hadamard_prod_plus.shape)
        #*****ADD MATRIX OF ERRORS HERE... (TO BE IMPLEMENTED)...
        # ****change the code below to draw from either the INV_CDF or a volume of zeros *****
        diff_dist_draw_plus = draw_3D_matrix_from_3D_diffdist(cdf_inverse_obj,Hadamard_prod_plus.shape[0],Hadamard_prod_plus.shape[1],Hadamard_prod_plus.shape[2])   #randm_3Dmat(cdf_inverse,Hadamard_prod.shape[0],Hadamard_prod.shape[1],Hadamard_prod.shape[2])    
        diff_dist_draw_minus =   draw_3D_matrix_from_3D_diffdist(cdf_inverse_obj,Hadamard_prod_minus.shape[0],Hadamard_prod_minus.shape[1],Hadamard_prod_minus.shape[2])

        # 
  #      print(np.max(diff_dist_draw_plus)) 
  #      print(np.max(diff_dist_draw_minus))
        # 
        
        Hadamard_with_error_plus = Hadamard_prod_plus + bernouli_ones*diff_dist_draw_plus
        Hadamard_with_error_minus = Hadamard_prod_minus + bernouli_ones*diff_dist_draw_minus
        # post process by multiplying the normalization factors and subtracting: X*W=(n_x)(n_w)(X_pre*W_+ - X_pre*W_-)
        Post_processed_Hadamard_with_error=n_x*n_w*(Hadamard_with_error_plus -  Hadamard_with_error_minus)  #  a*x_ijk_lateral_conv

        ##############################################################################################################################################
        # sum in channel dimension (outputs 2d array)
        Sum_in_channel_dim=np.sum(Post_processed_Hadamard_with_error, axis=0)  #np.sum(Hadamard_with_error,axis=0)
        # delete all zeros from the 2d array
        Three_d_conv_vol_nonzero=np.delete(Sum_in_channel_dim,list_of_indices_to_del,0)
        # reshape 2d array into LxLxD block
        Conv_block_3d=np.reshape(Three_d_conv_vol_nonzero,(-1,lateral_size,lateral_size))
        # now, sum in the lateral dimension. this turns the LxLxD array into a 1-d array
        Sum_in_lateral_dimension=np.sum(np.sum(Conv_block_3d,axis=1),axis=1)
        # reshape the 1d lateral summation into a 1x1xD block
        Three_d_block_to_append=np.reshape(Sum_in_lateral_dimension,(-1,1,1))
        # append to list
        stacked_conv_block_lst.append(Three_d_block_to_append)
  stacked_conv_block=np.hstack(stacked_conv_block_lst)    
  # reshape from [depth,H_temp,1] --> [depth,L,L] where L=sqrt(H)
  square_shape_L=int(np.sqrt(stacked_conv_block.shape[1]))
  lateral_square_conv_block=np.reshape(stacked_conv_block,(stacked_conv_block.shape[0],square_shape_L,square_shape_L))
  # reshape 3d conv block into 4d convblock keeping the lateral dimensions and infering the per-3d-output depths
  num_channels_for_output=x.shape[0]
  four_d_conv_block=np.reshape(lateral_square_conv_block,(num_channels_for_output,-1,lateral_square_conv_block.shape[1],lateral_square_conv_block.shape[2]))

  # CONVERT OUTPUT TO TENSOR....
  return torch.from_numpy(four_d_conv_block)

raw_scene_data=list(['/content/drive/My Drive/0:airport_inside', '/content/drive/My Drive/1:artstudio' , '/content/drive/My Drive/2:auditorium', '/content/drive/My Drive/3:bakery', 
                    '/content/drive/My Drive/4:bar', '/content/drive/My Drive/5:bathroom', '/content/drive/My Drive/6:bedroom', '/content/drive/My Drive/7:bookstore', 
                    '/content/drive/My Drive/8:bowling', '/content/drive/My Drive/9:buffet', '/content/drive/My Drive/10:casino', '/content/drive/My Drive/11:children_room', 
                     '/content/drive/My Drive/12:church_inside', '/content/drive/My Drive/13:classroom','/content/drive/My Drive/14:cloister','/content/drive/My Drive/15:closet', 
                     '/content/drive/My Drive/16:clothingstore', '/content/drive/My Drive/17:computerroom', '/content/drive/My Drive/18:concert_hall', '/content/drive/My Drive/19:corridor', 
                     '/content/drive/My Drive/20:deli', '/content/drive/My Drive/21:dentaloffice', '/content/drive/My Drive/22:dining_room', '/content/drive/My Drive/23:elevator', 
                     '/content/drive/My Drive/24:fastfood_restaurant', '/content/drive/My Drive/25:florist', '/content/drive/My Drive/26:gameroom', '/content/drive/My Drive/27:garage', 
                     '/content/drive/My Drive/28:greenhouse', '/content/drive/My Drive/29:grocerystore', '/content/drive/My Drive/30:gym','/content/drive/My Drive/31:hairsalon', 
                     '/content/drive/My Drive/32:hospitalroom', '/content/drive/My Drive/33:inside_bus', '/content/drive/My Drive/34:inside_subway', '/content/drive/My Drive/35:jewelleryshop', 
                     '/content/drive/My Drive/36:kindergarden', '/content/drive/My Drive/37:kitchen', '/content/drive/My Drive/38:laboratorywet', '/content/drive/My Drive/39:laundromat', 
                     '/content/drive/My Drive/40:library', '/content/drive/My Drive/41:livingroom', '/content/drive/My Drive/42:lobby', '/content/drive/My Drive/43:locker_room',
                     '/content/drive/My Drive/44:mall', '/content/drive/My Drive/45:meeting_room', '/content/drive/My Drive/46:movietheater', '/content/drive/My Drive/47:museum', 
                     '/content/drive/My Drive/48:nursery','/content/drive/My Drive/49:office', '/content/drive/My Drive/50:operating_room', '/content/drive/My Drive/51:pantry',
                     '/content/drive/My Drive/52:poolinside', '/content/drive/My Drive/53:prisoncell', '/content/drive/My Drive/54:restaurant', '/content/drive/My Drive/55:restaurant_kitchen',
                     '/content/drive/My Drive/56:shoeshop', '/content/drive/My Drive/57:stairscase', '/content/drive/My Drive/58:studiomusic', '/content/drive/My Drive/59:subway', 
                     '/content/drive/My Drive/60:toystore', '/content/drive/My Drive/61:trainstation', '/content/drive/My Drive/62:tv_studio', '/content/drive/My Drive/63:videostore',
                     '/content/drive/My Drive/64:waitingroom', '/content/drive/My Drive/65:warehouse','/content/drive/My Drive/66:winecellar'])

classes_to_use= list([15, 29, 4, 16, 56, 1, 46, 33, 18, 19, 59])   #list([3,34,65,54,10,59,4,0, 6,41,37])

for scene_class in  classes_to_use:
  print(raw_scene_data[scene_class])

num_classes=len(classes_to_use)

print(num_classes)

data_volume_by_class=[]

train_set_lst=[]
train_set_gt=[]

val_set_lst=[]
val_set_gt=[]

test_set_lst=[]
test_set_gt=[]

counter=0

#for scene in raw_scene_data:
for class_index in classes_to_use:
  scene=raw_scene_data[class_index]
  print("(raw) image class"+" "+str(class_index))
  raw_dataset= ImageDataset(scene,trans)
  L=len(raw_dataset)
  L_train= int(7*L/8)
  L_val= int(15*L/16) 
  data_volume_by_class.append(L)
  print("number of images:"+" "+str(L))
  raw_dataloader= torch.utils.data.DataLoader(raw_dataset, batch_size=1, shuffle=False, num_workers=0)  #note shuffle= FALSE here also, note the unit batch size to prevent stacking errors
  # GT class labels : [0, num_classes-1]
  gt_class= counter
  batch_counter=0
  for raw_image_batch in raw_dataloader:
    for raw_image in raw_image_batch: 
        # train list
      if 0<=batch_counter< L_train:
        train_images_appended=True
        train_set_lst.append(raw_image)
        train_set_gt.append(gt_class)
       # validation list
      elif L_train<=batch_counter<L_val:
        val_images_appended=True
        val_set_lst.append(raw_image)
        val_set_gt.append(gt_class)
       # testing list
      elif batch_counter>=L_val:
        test_images_appended=True
        test_set_lst.append(raw_image)
        test_set_gt.append(gt_class)    
      else:
        print("ERROR")
      batch_counter+=1


  if train_images_appended==False or val_images_appended==False or test_images_appended==False:
    print("ERROR: a train, val, or test image was not appended")

  
  counter+=1 
  if counter==1:
#    print("image size...")
#    print(raw_image.size())

    im=raw_dataset[0].permute(1,2,0).squeeze().detach().numpy()
    
    plt.imshow(im)
  


  print("=================")
 
  print("=================")
print("NOTE: 1's THAT SHOW UP REPRESENT ERRORS IN THE RAW DATA. THESE WILL BE FILTERED OUT IN SUBSEQUENT STEPS")

"""**Show sample data**"""

#print(train_set_gt)
#print(len(train_set_gt))
counter=0
for img in train_set_lst:
 # print(img.shape)
  if counter==0 or counter==1:
    plt.figure(figsize=(5,5))
    plt.imshow(img.permute(1,2,0))
  counter+=1

print("initial size of training set...")
print(len(train_set_lst))
print(len(train_set_gt))

print("initial size of validation set...")
print(len(val_set_lst))
print(len(val_set_gt))

print("initial size of test set...")
print(len(test_set_lst))
print(len(test_set_gt))

"""**Shuffled train indices**"""

train_indices_path='/content/drive/My Drive/train_indices'
#torch.save(train_indices,train_indices_path)
train_indices=torch.load(train_indices_path)
#np.random.shuffle(train_indices)
print(train_indices)
print(train_indices.shape)

"""**Shuffle Validation indices**"""

val_indices_path='/content/drive/My Drive/val_indices'
#torch.save(val_indices, val_indices_path) 
val_indices=torch.load(val_indices_path)
#np.random.shuffle(val_indices)
print(val_indices)
print(val_indices.shape)

"""** Test Indices**"""

test_indices=np.array([127, 106,  44,  97, 102, 116, 129, 143, 163,   0,   7,   1,  21,  63,  56,  15,  35, 156,
  17,  11,  40,  50,  34,   3,  66,  73, 87,   9,  53, 139, 119, 110, 121,  96,  95,   8,
 130,  71,  59, 58, 131, 137, 155,  68,  19, 112, 111,  84, 126,   4,  55,  99,  94, 107,
  46, 165,  48,  32, 115, 148,  57,  61,  60, 144, 103, 117, 135, 100,  64,  51,  79, 150,
  70, 140,  76,   6,  43,  45,  13,  75,  83,  33, 160,  78,  29,  77, 114, 153,  54,  31,
   5, 123, 162,  74, 101, 142, 124, 133,  12,  90,  39, 128,  38,  47, 136,  52,  93,  67,
 159,  14,  36,  42,  69, 104, 113, 132, 152,  30,  86, 146, 154, 147,  28, 109,  23,  22,
 164, 141, 105, 120, 151,  88,  41, 157,  26,  27, 108, 138,  85, 122, 134, 65,  81,  72,
  92,  16,  10,  18, 24, 161,  82,  98,  37,  91,  89,  62, 118,  25,   2,  49,  20,  80,
 125, 158, 149, 145])





print(test_indices)

"""**Compute Shuffled Training, Validation, and Testing Data and Filter out Corrupted Images**"""

shuffled_training_data=[]
shuffled_training_gt=[]

shuffled_validation_data=[]
shuffled_validation_gt=[]

shuffled_testing_data=[]
shuffled_testing_gt=[]

train_indicator=0
for index in train_indices:
  train_indicator+=1
#  if train_indicator<50:
#    print(index)

  drawn_element=train_set_lst[index]
  drawn_gt=train_set_gt[index]  # corresponding gt at same index
  # filter non-rgb images
  if drawn_element.size()[0]==3: 
    shuffled_training_data.append(drawn_element)
    shuffled_training_gt.append(drawn_gt)



val_indicator=0
for val_index in val_indices:
  val_indicator+=1
#  if val_indicator<50:
#    print(val_index)
  drawn_val_element=val_set_lst[val_index]
  drawn_val_gt=val_set_gt[val_index]  # corresponding gt at same index
  # filter non-rgb images
  if drawn_val_element.size()[0]==3: 
    shuffled_validation_data.append(drawn_val_element)
    shuffled_validation_gt.append(drawn_val_gt)

test_indicator=0
for test_index in test_indices:
  test_indicator+=1
#  if test_indicator<50:
#    print(test_index)
  drawn_test_element=test_set_lst[test_index]
  drawn_test_gt=test_set_gt[test_index]  # corresponding gt at same index
  # filter non-rgb images
  if drawn_test_element.size()[0]==3: 
    shuffled_testing_data.append(drawn_test_element)
    shuffled_testing_gt.append(drawn_test_gt)

"""**Grayscale all Images (run once)**"""

for img_train in range(len(shuffled_training_data)):
  if shuffled_training_data[img_train].size()[0]==3:
    shuffled_training_data[img_train]=transforms.Grayscale(1)(shuffled_training_data[img_train])

for img_val in range(len(shuffled_validation_data)):
  if shuffled_validation_data[img_val].size()[0]==3:
    shuffled_validation_data[img_val]=transforms.Grayscale(1)(shuffled_validation_data[img_val])
  
for img_test in range(len(shuffled_testing_data)):
  if shuffled_testing_data[img_test].size()[0]==3:
    shuffled_testing_data[img_test]=transforms.Grayscale(1)(shuffled_testing_data[img_test])

"""**Form the filtered datasets**"""

shuffled_train_dataset=PairedDataset(shuffled_training_data,shuffled_training_gt)
shuffled_validation_dataset=PairedDataset(shuffled_validation_data,shuffled_validation_gt)
shuffled_testing_datatset=PairedDataset(shuffled_testing_data,shuffled_testing_gt)


print("final size of training data...")
print(len(shuffled_train_dataset))
print("final size of validation data...")
print(len(shuffled_validation_dataset))
print("final size of testing data...")
print(len(shuffled_testing_datatset))

"""**form dataloaders from filtered datasets**"""

dataloaders = {
    'train': torch.utils.data.DataLoader(shuffled_train_dataset, batch_size=train_batchsize, shuffle=True, num_workers=0),
    'val': torch.utils.data.DataLoader(shuffled_validation_dataset, batch_size=val_batchsize, shuffle=False, num_workers=0),
    'test': torch.utils.data.DataLoader(shuffled_testing_datatset, batch_size=test_batchsize, shuffle=False, num_workers=0)
}

train_i=0
for image, gt in dataloaders['train']:
  train_i+=1
  if train_i==2:
    print(image.shape)
    print(gt)

val_i=0
for val_img, gt_v in dataloaders['val']:
  val_i+=1

test_i=0
for image_tst, gt_t in dataloaders['test']:
  test_i+=1

print("there are "+" "+str(train_i)+" "+"batches in the train data")
print("there are "+" "+str(val_i)+" "+"batches in the val data")
print("there are"+" "+str(test_i)+" "+"batches in the test data")

from typing import Type, Any, Callable, Union, List, Optional

import torch
import torch.nn as nn
from torch import Tensor
import math

"""**Construct RESNET**"""

class Bottleneck(nn.Module):
    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)
    # while original implementation places the stride at the first 1x1 convolution(self.conv1)
    # according to "Deep residual learning for image recognition"https://arxiv.org/abs/1512.03385.
    # This variant is also known as ResNet V1.5 and improves accuracy according to
    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.

    expansion: int = 4

    def __init__(
        self,
        inplanes: int,
        planes: int,
        stride: int = 1,
        downsample: Optional[nn.Module] = None,
        groups: int = 1,
        base_width: int = 64,
        dilation: int = 1,
        norm_layer: Optional[Callable[..., nn.Module]] = None,
    ) -> None:
        super().__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        width = int(planes * (base_width / 64.0)) * groups
        # Both self.conv2 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv1x1(inplanes, width)
        self.bn1 = norm_layer(width)
        self.conv2 = conv3x3(width, width, stride, groups, dilation)
        self.bn2 = norm_layer(width)
        self.conv3 = conv1x1(width, planes * self.expansion)
        self.bn3 = norm_layer(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x: Tensor) -> Tensor:
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out

def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:
    """3x3 convolution with padding"""
    return nn.Conv2d(
        in_planes,
        out_planes,
        kernel_size=3,
        stride=stride,
        padding=dilation,
        groups=groups,
        bias=False,
        dilation=dilation,
    )

def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)

class BasicBlock(nn.Module):
    expansion: int = 1

    def __init__(
        self,
        inplanes: int,
        planes: int,
        stride: int = 1,
        downsample: Optional[nn.Module] = None,
        groups: int = 1,
        base_width: int = 64,
        dilation: int = 1,
        norm_layer: Optional[Callable[..., nn.Module]] = None,
    ) -> None:
        super().__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        if groups != 1 or base_width != 64:
            raise ValueError("BasicBlock only supports groups=1 and base_width=64")
        if dilation > 1:
            raise NotImplementedError("Dilation > 1 not supported in BasicBlock")
        # Both self.conv1 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.bn1 = norm_layer(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = norm_layer(planes)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x: Tensor) -> Tensor:

        identity = x

        out1 = self.conv1(x)
        out2 = self.bn1(out1)
        out3 = self.relu(out2)

        out4 = self.conv2(out3)
        out5 = self.bn2(out4)

        if self.downsample is not None:

          identity = self.downsample(x)
        out5 += identity
        out = self.relu(out5)

        return out





class ResNet(nn.Module):
    def __init__(
        self,
        block: Type[Union[BasicBlock, Bottleneck]],
        layers: List[int],
        num_classes: int = 11,
        zero_init_residual: bool = False,
        groups: int = 1,
        width_per_group: int = 64,
        replace_stride_with_dilation: Optional[List[bool]] = None,
        norm_layer: Optional[Callable[..., nn.Module]] = None,
    ) -> None:
        super().__init__()
 #       _log_api_usage_once(self)
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        self._norm_layer = norm_layer

        self.inplanes = 64
        self.dilation = 1
        if replace_stride_with_dilation is None:
            # each element in the tuple indicates if we should replace
            # the 2x2 stride with a dilated convolution instead
            replace_stride_with_dilation = [False, False, False]
        if len(replace_stride_with_dilation) != 3:
            raise ValueError(
                "replace_stride_with_dilation should be None "
                f"or a 3-element tuple, got {replace_stride_with_dilation}"
            )
        self.groups = groups
        self.base_width = width_per_group
        self.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = norm_layer(self.inplanes)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * block.expansion, num_classes)
       
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

        # Zero-initialize the last BN in each residual branch,
        # so that the residual branch starts with zeros, and each residual block behaves like an identity.
        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677
        if zero_init_residual:
            for m in self.modules():
                if isinstance(m, Bottleneck):
                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]
                elif isinstance(m, BasicBlock):
                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]

    def _make_layer(
        self,
        block: Type[Union[BasicBlock, Bottleneck]],
        planes: int,
        blocks: int,
        stride: int = 1,
        dilate: bool = False,
    ) -> nn.Sequential:
        norm_layer = self._norm_layer
        downsample = None
        previous_dilation = self.dilation
        if dilate:
            self.dilation *= stride
            stride = 1
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                conv1x1(self.inplanes, planes * block.expansion, stride),
                norm_layer(planes * block.expansion),
            )

        layers = []
        layers.append(
            block(
                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer
            )
        )
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(
                block(
                    self.inplanes,
                    planes,
                    groups=self.groups,
                    base_width=self.base_width,
                    dilation=self.dilation,
                    norm_layer=norm_layer,
                )
            )

        return nn.Sequential(*layers)

    def _forward_impl(self, x: Tensor) -> Tensor:
        # See note [TorchScript super()]
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)

        return x

    def forward(self, x: Tensor) -> Tensor:
        return self._forward_impl(x)


def _resnet(
    arch: str,
    block: Type[Union[BasicBlock, Bottleneck]],
    layers: List[int],
    pretrained: bool,
    progress: bool,
    **kwargs: Any,
) -> ResNet:
    model = ResNet(block, layers, **kwargs)
    if pretrained:
        print("loading weight variables...")
        state_dict = VGG_F.state_dict()  #load_state_dict_from_url(model_urls[arch], progress=progress)
        print("initial state dict...")
        for param_tensor in model.state_dict():
          print(param_tensor)
        model.load_state_dict(state_dict)
        
    return model

from typing import Union, List, Dict, Any, cast

import torch
import torch.nn as nn

"""**Construct VGG Net**"""

class VGG(nn.Module):
    def __init__(
        self, features: nn.Module, num_classes: int = 11, init_weights: bool = True, dropout: float = 0.5
    ) -> None:
        super().__init__()
  #      _log_api_usage_once(self)
        self.features = features
        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))
        self.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(True),
            nn.Dropout(p=dropout),
            nn.Linear(4096, 4096),
            nn.ReLU(True),
            nn.Dropout(p=dropout),
            nn.Linear(4096, num_classes),
        )
        if init_weights:
            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)
                elif isinstance(m, nn.BatchNorm2d):
                    nn.init.constant_(m.weight, 1)
                    nn.init.constant_(m.bias, 0)
                elif isinstance(m, nn.Linear):
                    nn.init.normal_(m.weight, 0, 0.01)
                    nn.init.constant_(m.bias, 0)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.features(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x


def make_layers(cfg: List[Union[str, int]], batch_norm: bool = False) -> nn.Sequential:
    layers: List[nn.Module] = []
    in_channels = 1
    for v in cfg:
        if v == "M":
            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
        else:
            v = cast(int, v)
            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)
            if batch_norm:
                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]
            else:
                layers += [conv2d, nn.ReLU(inplace=True)]
            in_channels = v
    return nn.Sequential(*layers)


cfgs: Dict[str, List[Union[str, int]]] = {
    "A": [64, "M", 128, "M", 256, 256, "M", 512, 512, "M", 512, 512, "M"],
    "B": [64, 64, "M", 128, 128, "M", 256, 256, "M", 512, 512, "M", 512, 512, "M"],
    "D": [64, 64, "M", 128, 128, "M", 256, 256, 256, "M", 512, 512, 512, "M", 512, 512, 512, "M"],
    "E": [64, 64, "M", 128, 128, "M", 256, 256, 256, 256, "M", 512, 512, 512, 512, "M", 512, 512, 512, 512, "M"],
}


def _vgg(arch: str, cfg: str, batch_norm: bool, pretrained: bool, progress: bool, **kwargs: Any) -> VGG:
    if pretrained:
        kwargs["init_weights"] = False
    model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs)
    if pretrained:
        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)
        model.load_state_dict(state_dict)
    return model

def vgg11(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> VGG:
    r"""VGG 11-layer model (configuration "A") from
    `"Very Deep Convolutional Networks For Large-Scale Image Recognition" <https://arxiv.org/pdf/1409.1556.pdf>`_.
    The required minimum input size of the model is 32x32.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    return _vgg("vgg11", "A", False, pretrained, progress, **kwargs)

def vgg11_bn(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> VGG:
    r"""VGG 11-layer model (configuration "A") with batch normalization
    `"Very Deep Convolutional Networks For Large-Scale Image Recognition" <https://arxiv.org/pdf/1409.1556.pdf>`_.
    The required minimum input size of the model is 32x32.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    return _vgg("vgg11_bn", "A", True, pretrained, progress, **kwargs)

def vgg13(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> VGG:
    r"""VGG 13-layer model (configuration "B")
    `"Very Deep Convolutional Networks For Large-Scale Image Recognition" <https://arxiv.org/pdf/1409.1556.pdf>`_.
    The required minimum input size of the model is 32x32.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    return _vgg("vgg13", "B", False, pretrained, progress, **kwargs)

def vgg13_bn(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> VGG:
    r"""VGG 13-layer model (configuration "B") with batch normalization
    `"Very Deep Convolutional Networks For Large-Scale Image Recognition" <https://arxiv.org/pdf/1409.1556.pdf>`_.
    The required minimum input size of the model is 32x32.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    return _vgg("vgg13_bn", "B", True, pretrained, progress, **kwargs)

def vgg16(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> VGG:
    r"""VGG 16-layer model (configuration "D")
    `"Very Deep Convolutional Networks For Large-Scale Image Recognition" <https://arxiv.org/pdf/1409.1556.pdf>`_.
    The required minimum input size of the model is 32x32.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    return _vgg("vgg16", "D", False, pretrained, progress, **kwargs)

def vgg16_bn(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> VGG:
    r"""VGG 16-layer model (configuration "D") with batch normalization
    `"Very Deep Convolutional Networks For Large-Scale Image Recognition" <https://arxiv.org/pdf/1409.1556.pdf>`_.
    The required minimum input size of the model is 32x32.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    return _vgg("vgg16_bn", "D", True, pretrained, progress, **kwargs)

def vgg19(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> VGG:
    r"""VGG 19-layer model (configuration "E")
    `"Very Deep Convolutional Networks For Large-Scale Image Recognition" <https://arxiv.org/pdf/1409.1556.pdf>`_.
    The required minimum input size of the model is 32x32.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    return _vgg("vgg19", "E", False, pretrained, progress, **kwargs)

def vgg19_bn(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> VGG:
    r"""VGG 19-layer model (configuration 'E') with batch normalization
    `"Very Deep Convolutional Networks For Large-Scale Image Recognition" <https://arxiv.org/pdf/1409.1556.pdf>`_.
    The required minimum input size of the model is 32x32.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    return _vgg("vgg19_bn", "E", True, pretrained, progress, **kwargs)

sample_input=torch.ones([train_batchsize,1,224,224])
model=vgg11_bn() 
out=model(sample_input)
print(out.shape)

#for param_tensor in model.state_dict():
#  print(param_tensor)

"""**Train Model**"""

def train_model(model, optimizer,scheduler, num_epochs):
    # for each epoch... 
    for epoch in trange(num_epochs):
     
      start_time = time()
      train_loss = 0
      print('Epoch {}/{}'.format(epoch, num_epochs - 1))
      print('-' * 10)
      # let every epoch go through one training cycle and one validation cycle
      # TRAINING AND THEN VALIDATION LOOP...
      for phase in ['train', 'val']:
        # SELECT PROPER MODE- train or val
        if phase == 'train':
          for param_group in optimizer.param_groups:
            print("LR", param_group['lr']) # print out the learning rate
          model.train()  # Set model to training mode
        else:
          model.eval()   # Set model to evaluate mode
# having selected the appropriate mode, we then proceed... 
        for inputs, labels in dataloaders[phase]:  
          # NOTE: LABELS MUST BE INDEXED TO START AT 0 SO THEY GO FROM [0, NUM_CLASSES-1]
          # TO CONVERT FROM LABELS TO PREDICTIONS, USE: PRED=LABEL+1 
          labels=labels       
# clear out the current optimizer gradients 
          optimizer.zero_grad()
# enable gradient tracking through training phase ONLY
          with torch.set_grad_enabled(phase == 'train'):
            outputs = model(inputs)#.type(torch.float32)
 #           print("=============")
 #           print(outputs.shape)
 #           print(labels)
 #           print("===========")
            loss = nn.CrossEntropyLoss()(outputs, labels) #.type(torch.float32)  torch.tensor(1.2, requires_grad=True) #
            ######################################################
            predictions=torch.argmax(outputs,dim=1)
            predictions=list(predictions)

            epoch_acc=[]
            for j in range(len(predictions)):
              out_pred=predictions[j]
              target_pred=labels[j]
              if out_pred==target_pred:
                epoch_acc.append(1)

            if phase=='val':
              print("per-epoch test accuracy...")
              print(len(epoch_acc)/len(predictions))

            ######################################################        
            # backward + optimize only if in training phase
            if phase == 'train':
              loss.backward()
              optimizer.step()
            train_loss += loss
        
        
        if phase == 'train':
          scheduler.step()
        # prints for training and then validation (since the network will be in either train or eval mode at this point) 
        if phase == 'train':
          print(" Training Epoch %d, Total loss %0.6f, iteration time %0.6f" % (epoch, train_loss, time() - start_time))
      
#          print(loss)
        if phase == 'val':
          print(" Validation Epoch %d, Total loss %0.6f, iteration time %0.6f" % (epoch, train_loss, time() - start_time))
      
      

    # end of single epoch iteration... repeat of n epochs  
    return model

#num_epochs=1
#model=vgg11_bn() 
#loss_fn=nn.CrossEntropyLoss()
#optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
#scheduler=torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

"""**Load Model**"""

path_to_load= '/content/drive/My Drive/fixed_index_resnet18_11class_RESOLUTION_168_30_EPOCH.pt' 



#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


#'/content/drive/My Drive/fixed_indices_vgg11_11class_RESOLUTION_168_30_EPOCH.pt'
#'/content/drive/My Drive/fixed_indices_vgg11_star_11class.pt'
# '/content/drive/My Drive/fixed_indices_vgg13_11class.pt' 
# '/content/drive/My Drive/30epoch_vgg13_star_1class.pt' 
#'/content/drive/My Drive/fixed_indices_vgg16_11class_RESOLUTION_168_30_EPOCH.pt' 
#'/content/drive/My Drive/fixed_indices_vgg16_star_11class.pt'
#'/content/drive/My Drive/fixed_indices_vgg19_11class.pt'  
#'/content/drive/My Drive/fixed_index_resnet18_11class_RESOLUTION_168_30_EPOCH.pt' 
#'/content/drive/My Drive/fixed_indices_resnet34_11class_RESOLUTION_168_30_EPOCH.pt'
#'/content/drive/My Drive/fixed_indices_resnet50_11class_RESOLUTION_168_30_EPOCH.pt' #
 
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

VGG_F=torch.load(path_to_load)
VGG_F.eval()

"""**Construct Testing Function for Model**"""

testing_batches=5

def test_VGG_model(model):
  '''
  tests a convolutional model
  INPUT: a model of the form y=f(x) that maps an input to an output
  '''
  output_lst=[]
  gt_lst=[]
  test_counter=0
  acc_per_batch=[]
  for test_batch,test_gt in dataloaders['test']:
 #   print(test_batch.size())
    print("batch number"+" "+str(test_counter)+" "+"of"+" "+str(testing_batches))
    test_counter +=1
    if test_counter>testing_batches:
        break
    # forward pass
    output=model(test_batch)
    # class predictions
    final_out=torch.argmax(output, dim=1).numpy()
    print(final_out)
    print(test_gt)
    output_lst.append(final_out)
    gt=test_gt.numpy()
    gt_lst.append(gt)
    #============= (OPTIONAL)=====================
    # first, look at the per-batch accuracy level
    current_acc=[]
    for k in range(len(final_out)):
      if final_out[k]==gt[k]: 
        current_acc.append(1)
    batch_accuracy=len(current_acc)/len(final_out)
    acc_per_batch.append(batch_accuracy)
    #=============================================
 # output_lst is a list containing all output class predictions as [Batch_1_pred, Batch_2_pred,...]
 # horizontally stack the outputs and ground truths into single arrays
  output_array=np.hstack(output_lst)
  gt_array=np.hstack(gt_lst)
  # compare the stacked output array with the stacked ground truth array
  correct_predictions=[]
  for i in range(len(output_array)):
    output_elem=output_array[i]
    gt_elem=gt_array[i]
    if output_elem==gt_elem:
      correct_predictions.append(1)
  
  accuracy=len(correct_predictions)/len(output_array)
  return  accuracy  # accuracy is the overall averaged accuracy over all outputs, acc_per_batch is a list of per-batch accuracies (should average to variable accuracy)

digital_model_to_test = VGG_F
digital_test=test_VGG_model(digital_model_to_test)

print(digital_test)

"""**Access the difference distribution**"""

mat_path='/content/drive/My Drive/distribution_data.mat'
storage = loadmat(mat_path)
storage = storage['dd']
to_plot_storage = storage.flatten()

if draw_errors==True:
  save_path='/content/drive/My Drive/error_vol_for_UNET.pt'
  volume=torch.load(save_path)
  CONV_Diff_Vol=np.reshape(volume,(2000, 5000,10))

  to_stack=[]
  for i in range(5):
    tensors_to_cat=np.concatenate((CONV_Diff_Vol,CONV_Diff_Vol),axis=2)
    to_stack.append(tensors_to_cat)

  deep_stacked_array=np.stack(to_stack, axis=2)

if draw_errors==False:
  save_path_zero='/content/drive/My Drive/zeros_for_UNET.pt'
  zero_vol=torch.load(save_path_zero)
  CONV_Diff_Vol_zero=np.reshape(zero_vol,(2000, 5000,10))

  to_stack=[]
  for i in range(5):
    tensors_to_cat=np.concatenate((CONV_Diff_Vol_zero,CONV_Diff_Vol_zero),axis=2)
    to_stack.append(tensors_to_cat)

  deep_stacked_array=np.stack(to_stack, axis=2)


print(deep_stacked_array.shape)

difference_distribution=deep_stacked_array
difference_distribution=np.reshape(difference_distribution,(512,-1,5))
C1_DIFF=np.reshape(difference_distribution,(100,-1,10))

sd=VGG_F.state_dict()

for param_tensor in sd:
  print(param_tensor)

print(difference_distribution.shape)
print(C1_DIFF.shape)

print(np.max(difference_distribution))
print(np.max(C1_DIFF))

"""**Make RESNET that passes the convolutional weight matrices into the forward method of the Basic Blocks and first 2d convolution**"""

def conv3x3_local(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:
    """3x3 convolution with padding"""
    return nn.Conv2d(
        in_planes,
        out_planes,
        kernel_size=3,
        stride=stride,
        padding=dilation,
        groups=groups,
        bias=False,
        dilation=dilation,
    )

def conv1x1_local(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)

class BasicBlock_local(nn.Module):
    expansion: int = 1

    def __init__(
        self,
        inplanes: int,
        planes: int,
        difference_distribution, 
        sd,
        layer_index:int,
        stride: int = 1,
        downsample: Optional[nn.Module] = None,
        groups: int = 1,
        base_width: int = 64,
        dilation: int = 1,
        norm_layer: Optional[Callable[..., nn.Module]] = None,
    ) -> None:
        super().__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        if groups != 1 or base_width != 64:
            raise ValueError("BasicBlock only supports groups=1 and base_width=64")
        if dilation > 1:
            raise NotImplementedError("Dilation > 1 not supported in BasicBlock")
        # Both self.conv1 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv3x3_local(inplanes, planes, stride)
        self.bn1 = norm_layer(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3_local(planes, planes)
        self.bn2 = norm_layer(planes)
        self.downsample = downsample
        self.stride = stride


        #######################################
        self.difference_distribution = difference_distribution
        self.sd=  sd
        self.layer_index=layer_index
 #       print(np.min(self.difference_distribution))
 #       print(np.max(self.difference_distribution))
        #######################################

    def forward(self, x: Tensor) -> Tensor:
  #      print("beginning res block...")
  #      print(self.layer_index)
  #      print('===========================================')

# use the layer index to determine what weights to use
#***********************************************
        if self.layer_index==0:
          w1=self.sd['layer1.0.conv1.weight']
          w2=self.sd['layer1.0.conv2.weight']
        elif self.layer_index==1:
          w1=self.sd['layer1.1.conv1.weight']
          w2=self.sd['layer1.1.conv2.weight']
        elif self.layer_index==2:
          w1=self.sd['layer2.0.conv1.weight']
          w2=self.sd['layer2.0.conv2.weight']
        elif self.layer_index==3:
          w1=self.sd['layer2.1.conv1.weight']
          w2=self.sd['layer2.1.conv2.weight']
        elif self.layer_index==4:
          w1=self.sd['layer3.0.conv1.weight']
          w2=self.sd['layer3.0.conv2.weight']
        elif self.layer_index==5:
          w1=self.sd['layer3.1.conv1.weight']
          w2=self.sd['layer3.1.conv2.weight']
        elif self.layer_index==6: 
          w1=self.sd['layer4.0.conv1.weight']
          w2= self.sd['layer4.0.conv2.weight']
        elif self.layer_index==7:
          w1=self.sd['layer4.1.conv1.weight']
          w2=self.sd['layer4.1.conv2.weight']       
#***********************************************             

        identity = x

        
        out1_gt = self.conv1(x)
        l_c1_s=self.conv1.stride
        l_c1_p=self.conv1.padding
        l_c1_d=self.conv1.dilation
        out1=ADV_Conv_4D_With_Processing(self.difference_distribution,x,w1,l_c1_s[0],l_c1_p[0],l_c1_p[1],l_c1_d[0],l_c1_d[1]).type(torch.FloatTensor)

        print(torch.max(out1_gt-out1))
       
        out2 = self.bn1(out1)
        out3 = self.relu(out2)

        out4_gt = self.conv2(out3)
        l_c2_s =self.conv2.stride
        l_c2_p =self.conv2.padding
        l_c2_d =self.conv2.dilation
        out4=ADV_Conv_4D_With_Processing(self.difference_distribution,out3,w2,l_c2_s[0], l_c2_p[0],l_c2_p[1],l_c2_d[0],l_c2_d[1]).type(torch.FloatTensor)

      #  print(torch.max(out4_gt- out4))

        out5 = self.bn2(out4)

        if self.downsample is not None:

          # use the layer index to determine what weights to use
          #***********************************************
          if self.layer_index==2:
            w_ds=self.sd['layer2.0.downsample.0.weight']        
          elif self.layer_index==4:
            w_ds=self.sd['layer3.0.downsample.0.weight']         
          elif self.layer_index==6: 
            w_ds=self.sd['layer4.0.downsample.0.weight']
          #***********************************************

          identity_gt = self.downsample[0](x)
          l_d_s=self.downsample[0].stride
          l_d_p=self.downsample[0].padding
          l_d_d=self.downsample[0].dilation
          identity=ADV_Conv_4D_With_Processing(self.difference_distribution,x,w_ds, l_d_s[0],l_d_p[0],l_d_p[1],l_d_d[0],l_d_d[1]).type(torch.FloatTensor)

          print(torch.max( identity_gt-identity))

          identity=self.downsample[1](identity)

        out5 += identity
        out = self.relu(out5)


        return out





class ResNet_local(nn.Module):
    def __init__(
        self,
        block: Type[Union[BasicBlock_local, Bottleneck]],
        layers: List[int],
        C1_DIFF, 
        difference_distribution, 
        sd,
        num_classes: int = 11,
        zero_init_residual: bool = False,
        groups: int = 1,
        width_per_group: int = 64,
        replace_stride_with_dilation: Optional[List[bool]] = None,
        norm_layer: Optional[Callable[..., nn.Module]] = None,
    ) -> None:
        super().__init__()
 #       _log_api_usage_once(self)
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        self._norm_layer = norm_layer

        self.inplanes = 64
        self.dilation = 1
        if replace_stride_with_dilation is None:
            # each element in the tuple indicates if we should replace
            # the 2x2 stride with a dilated convolution instead
            replace_stride_with_dilation = [False, False, False]
        if len(replace_stride_with_dilation) != 3:
            raise ValueError(
                "replace_stride_with_dilation should be None "
                f"or a 3-element tuple, got {replace_stride_with_dilation}"
            )


        ######################################################################################     
        self.C1_DIFF = C1_DIFF
        self.difference_distribution = difference_distribution
        self.sd = sd

        ######################################################################################

        self.groups = groups
        self.base_width = width_per_group
        self.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = norm_layer(self.inplanes)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer_local(block, 64, layers[0],difference_distribution, 
        sd,0,1)
        self.layer2 = self._make_layer_local(block, 128, layers[1], self.difference_distribution, self.sd, 2,3, stride=2, dilate=replace_stride_with_dilation[0])
        self.layer3 = self._make_layer_local(block, 256, layers[2], self.difference_distribution, self.sd, 4,5, stride=2, dilate=replace_stride_with_dilation[1])
        self.layer4 = self._make_layer_local(block, 512, layers[3], self.difference_distribution, self.sd, 6,7, stride=2, dilate=replace_stride_with_dilation[2])
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * block.expansion, num_classes)




        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

        # Zero-initialize the last BN in each residual branch,
        # so that the residual branch starts with zeros, and each residual block behaves like an identity.
        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677
        if zero_init_residual:
            for m in self.modules():
                if isinstance(m, Bottleneck):
                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]
                elif isinstance(m, BasicBlock):
                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]

    def _make_layer_local(
        self,
        block: Type[Union[BasicBlock_local, Bottleneck]],
        planes: int,
        blocks: int,

        difference_distribution, 
        sd,
        layer_index1: int,
        layer_index2: int,
        stride: int = 1,
        dilate: bool = False,
        
    ) -> nn.Sequential:
        norm_layer = self._norm_layer
        downsample = None
        previous_dilation = self.dilation
        if dilate:
            self.dilation *= stride
            stride = 1
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                conv1x1_local(self.inplanes, planes * block.expansion, stride),
                norm_layer(planes * block.expansion),
            )

        layers = []
        layers.append(
            block(
                self.inplanes, planes, difference_distribution, sd, layer_index1, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer, 
                
            )
        )
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(
                block(
                    self.inplanes,
                    planes,
                    difference_distribution, 
                    sd,
                    layer_index2,
                    groups=self.groups,
                    base_width=self.base_width,
                    dilation=self.dilation,
                    norm_layer=norm_layer,
                    
                )
            )

        return nn.Sequential(*layers)

    def _forward_impl(self, x: Tensor) -> Tensor:
        # See note [TorchScript super()]
        print("beginning forward pass...")
        print()
        print('========================================')
        W_conv=self.sd['conv1.weight']
        W3=self.sd['fc.weight']
        W3_bias=self.sd['fc.bias']
        two_d_diff_dist=self.difference_distribution.reshape([self.difference_distribution.shape[1],self.difference_distribution.shape[0]*self.difference_distribution.shape[2]])
             
        c1_gt = self.conv1(x)
        c1_s=self.conv1.stride
        c1_p=self.conv1.padding
        c1_d=self.conv1.dilation
        c1=ADV_Conv_4D_With_Processing(self.C1_DIFF,x,W_conv,c1_s[0],c1_p[0],c1_p[1],c1_d[0],c1_d[1]).type(torch.FloatTensor)

        print(torch.max(c1_gt-c1))

        bn1 = self.bn1(c1)
        r1 = self.relu(bn1)
        m1 = self.maxpool(r1)

        l1 = self.layer1(m1)
        l2 = self.layer2(l1)
        l3 = self.layer3(l2)
        l4 = self.layer4(l3)

        a1 = self.avgpool(l4)
        x_flat = torch.flatten(a1, 1)
        ################################################
        n_x1=torch.max(x_flat)
        c1=torch.min(W3)
        d1=torch.max(W3)
        n_w1=max(np.abs(c1),np.abs(d1))
        x_p1 = x_flat/n_x1
        w_p1 = W3/n_w1
        w_pos1, w_neg1 = quick_split_W3D(w_p1)
        Prod_plus1=Matmul_with_2d_diffdist_elements(two_d_diff_dist,x_p1.detach().numpy(),w_pos1) 
        Prod_minus1=Matmul_with_2d_diffdist_elements(two_d_diff_dist,x_p1.detach().numpy(),w_neg1)
        Post_Processed1 = n_x1*n_w1*(Prod_plus1- Prod_minus1) 
        FC_out=torch.tensor(Post_Processed1) + W3_bias
################################################
        x_out_gt=self.fc(x_flat)
        x_out = FC_out 

        print(torch.max(x_out_gt-x_out))
        return x_out

    def forward(self, x: Tensor) -> Tensor:
        return self._forward_impl(x)


def _resnet_local(
    arch: str,
    block: Type[Union[BasicBlock_local, Bottleneck]],
    layers: List[int],
    pretrained: bool,
    progress: bool,
    C1_DIFF, 
    difference_distribution, 
    sd,
    **kwargs: Any,
) -> ResNet_local:
    model = ResNet_local(block, layers, C1_DIFF, difference_distribution, sd, **kwargs)
    if pretrained:
        print("loading weight variables...")
        state_dict = VGG_F.state_dict()  #load_state_dict_from_url(model_urls[arch], progress=progress)
        print("initial state dict...")
       # print('=============================================')
        #for param_tensor in model.state_dict():
         # print(param_tensor)
        model.load_state_dict(state_dict)
        print("all parameters loaded")
        
    return model

def resnet18_local(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet_local:
    r"""ResNet-18 model from
    `"Deep Residual Learning for Image Recognition" <https://arxiv.org/pdf/1512.03385.pdf>`_.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    return _resnet_local("resnet18", BasicBlock_local, [2, 2, 2, 2], pretrained, progress, C1_DIFF, difference_distribution, sd, **kwargs)

local_model=resnet18_local(pretrained=True).eval()
#local_test=test_VGG_model(local_model)
#print(local_test)

test_batch_list=[]
test_gt_list=[]
for test_batch,test_gt in dataloaders['test']: 
  test_batch_list.append(test_batch)
  test_gt_list.append(test_gt)


print(test_batch_list[0].shape)
print(test_gt_list[0].shape)

test_batch=test_batch_list[0]  # the test_batch indices can be altered to change test input over the total number of batches 
test_gt=test_gt_list[0]

output=local_model(test_batch)
# class predictions
final_out=torch.argmax(output, dim=1).numpy()
print(final_out)
print(test_gt)
gt=test_gt.numpy()
  
current_acc=[]
for k in range(len(final_out)):
  if final_out[k]==gt[k]: 
    current_acc.append(1)

percent_acc=len(current_acc)/len(final_out)

print("model one-batch accuracy is"+" "+str(percent_acc))